{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13094586,"sourceType":"datasetVersion","datasetId":8294227}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# a. BASIC MACHINE TRANSLATION USING RULE-BASED METHODS","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"dictionary = { \n    'hello': 'bonjour', \n    'world': 'monde', \n    'my': 'mon', \n    'name': 'nom', \n    'is': 'est' \n}\ngrammar_rules = { \n    'SVO': ['subject', 'verb', 'object'] \n}  \ndef translate(sentence): \n    words = sentence.lower().split() \n    translated_words = [dictionary.get(word, word) for word in words] \n    return ' '.join(translated_words) \nsentence = \"Hello world\" \nprint(translate(sentence))  \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T11:00:37.227654Z","iopub.execute_input":"2025-09-10T11:00:37.227957Z","iopub.status.idle":"2025-09-10T11:00:37.235987Z","shell.execute_reply.started":"2025-09-10T11:00:37.227935Z","shell.execute_reply":"2025-09-10T11:00:37.234381Z"}},"outputs":[{"name":"stdout","text":"bonjour monde\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# b. ENGLISH TO FRENCH TRANSLATION USING SEQ2SEQ WITH ATTENTION","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Attention\nimport numpy as np\n\n# --- Sample Dataset ---\neng = [\"i like apples\", \"they do not like grapefruit\"]\nfr  = [\"j aime les pommes\", \"ils n aiment pas le pamplemousse\"]\n\neng_vocab = {w:i+1 for i,w in enumerate(set(\" \".join(eng).split()))}\nfr_vocab  = {w:i+1 for i,w in enumerate(set(\" \".join(fr).split()))}\ninv_fr = {i:w for w,i in fr_vocab.items()}\n\ndef encode(s, vocab, max_len=6): \n    return [vocab[w] for w in s.split()]+[0]*(max_len-len(s.split()))\n\nX = np.array([encode(s,eng_vocab) for s in eng])\nY = np.array([encode(s,fr_vocab) for s in fr])\n\n# --- Seq2Seq with Attention ---\nclass Seq2Seq(tf.keras.Model):\n    def __init__(self, in_vocab, out_vocab, emb=16, units=32):\n        super().__init__()\n        self.enc_emb = Embedding(in_vocab+1, emb)\n        self.enc_lstm = LSTM(units, return_sequences=True, return_state=True)\n        self.dec_emb = Embedding(out_vocab+1, emb)\n        self.dec_lstm = LSTM(units, return_sequences=True, return_state=True)\n        self.att, self.fc = Attention(), Dense(out_vocab+1, activation=\"softmax\")\n\n    def call(self, inputs):\n        x, y = inputs  # unpack\n        enc_out,h,c = self.enc_lstm(self.enc_emb(x))\n        dec_out,_,_ = self.dec_lstm(self.dec_emb(y), initial_state=[h,c])\n        ctx = self.att([dec_out, enc_out])\n        return self.fc(tf.concat([dec_out, ctx], -1))\n\nmodel = Seq2Seq(len(eng_vocab), len(fr_vocab))\nmodel.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\")\nmodel.fit([X,Y], np.expand_dims(Y,-1), epochs=100, verbose=0)\n\n# --- Translation ---\ntest = np.array([encode(\"they do not like grapefruit\", eng_vocab)])\nstart = np.array([encode(\"ils\", fr_vocab)])  # seed word\npred = np.argmax(model.predict([test,start])[0],-1)\nprint(\"Translation:\", \" \".join(inv_fr.get(i,\"\") for i in pred))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T12:27:23.384539Z","iopub.execute_input":"2025-09-24T12:27:23.384879Z","iopub.status.idle":"2025-09-24T12:27:36.166997Z","shell.execute_reply.started":"2025-09-24T12:27:23.384853Z","shell.execute_reply":"2025-09-24T12:27:36.166046Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 457ms/step\nTranslation: ils n aiment pas le pamplemousse\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# c. NEURAL MACHINE TRANSLATION WITH TRANSFORMERS (ENGLISH TO GERMAN) (google Collab)","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\nmodel = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")\nchat_history_ids = None\nprint(\" Conversational AI Chatbot (type 'quit' to exit)\")\nwhile True:\n    user_input = input(\"You: \")\n    if user_input.lower() in [\"quit\", \"exit\"]:\n        print(\"Bot: Goodbye! üëã\")\n        break\n    new_input_ids = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors='pt')\n    if chat_history_ids is not None:\n        bot_input_ids = torch.cat([chat_history_ids, new_input_ids], dim=-1)\n    else:\n        bot_input_ids = new_input_ids\n    chat_history_ids = model.generate(\n        bot_input_ids,\n        max_length=1000,\n        pad_token_id=tokenizer.eos_token_id,\n        do_sample=True,\n        temperature=0.7,\n        top_k=50,\n        top_p=0.95\n    )\n    bot_output = tokenizer.decode(\n        chat_history_ids[:, bot_input_ids.shape[-1]:][0],\n        skip_special_tokens=True\n    )\n    print(\"Bot:\", bot_output)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-09-24T13:20:52.276Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}