{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3176,"sourceType":"datasetVersion","datasetId":1835}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# A. BASIC WORD EMBEDDINGS WITH TF-IDF","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2025-09-17T12:07:52.766141Z","iopub.execute_input":"2025-09-17T12:07:52.766417Z","iopub.status.idle":"2025-09-17T12:07:54.388018Z","shell.execute_reply.started":"2025-09-17T12:07:52.766392Z","shell.execute_reply":"2025-09-17T12:07:54.386966Z"},"editable":false}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer  \n\ncorpus = [ \n    \"Deep learning is fun\", \n    \"Word embeddings can be learned\", \n    \"TF-IDF captures word importance\", \n    \"Embeddings represent words as vectors\" \n] \n\nvectorizer = TfidfVectorizer() \nX = vectorizer.fit_transform(corpus) \n\nfeatures = vectorizer.get_feature_names_out()\ntfidf_matrix = X.toarray() \n\nprint(\"TF-IDF Feature Names:\") \nprint(features) \nprint(\"\\nTF-IDF Matrix:\") \nprint(tfidf_matrix) \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T04:28:44.354992Z","iopub.execute_input":"2025-09-18T04:28:44.355405Z","iopub.status.idle":"2025-09-18T04:28:44.393510Z","shell.execute_reply.started":"2025-09-18T04:28:44.355378Z","shell.execute_reply":"2025-09-18T04:28:44.392652Z"},"editable":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# B. GENERATING WORD EMBEDDINGS USING WORD2VEC AND GLOVE","metadata":{"editable":false}},{"cell_type":"code","source":"from gensim.utils import simple_preprocess\nfrom gensim.models import Word2Vec, KeyedVectors\nfrom gensim.scripts.glove2word2vec import glove2word2vec\ncorpus = [\n    \"Natural language processing enables computers to understand human language.\",\n    \"Word embeddings capture semantic relationships between words in a vector space.\",\n    \"Deep learning techniques such as Word2Vec and GloVe are widely used in NLP applications.\",\n    \"This is a sample document for generating word embeddings.\",\n    \"Another example document is provided for demonstration purposes.\"\n]\ntokenized_corpus = [simple_preprocess(sentence) for sentence in corpus]\nprint(\"Sample tokenized sentences:\\n\", tokenized_corpus)\nprint(\"\\nTraining Word2Vec model...\")\nw2v_model = Word2Vec(sentences=tokenized_corpus, vector_size=100, window=5, sg=1, min_count=1, workers=4)\n\nprint(\"\\nWord2Vec: Similar words to 'document'\")\nprint(w2v_model.wv.most_similar(\"document\", topn=5))\nprint(\"\\nLoading GloVe embeddings...\")\nglove_file = \"glove.6B.100d.txt\"\nglove2word2vec(glove_file, \"glove.6B.100d.word2vec.txt\")\nglove_model = KeyedVectors.load_word2vec_format(\"glove.6B.100d.word2vec.txt\", binary=False)\nprint(\"\\nGloVe: Similar words to 'document'\")\nprint(glove_model.most_similar(\"document\", topn=5))\n","metadata":{"trusted":true,"editable":false,"execution":{"iopub.status.busy":"2025-09-24T15:31:38.803819Z","iopub.execute_input":"2025-09-24T15:31:38.804160Z","iopub.status.idle":"2025-09-24T15:32:17.917235Z","shell.execute_reply.started":"2025-09-24T15:31:38.804135Z","shell.execute_reply":"2025-09-24T15:32:17.915912Z"}},"outputs":[{"name":"stdout","text":"Sample tokenized sentences:\n [['natural', 'language', 'processing', 'enables', 'computers', 'to', 'understand', 'human', 'language'], ['word', 'embeddings', 'capture', 'semantic', 'relationships', 'between', 'words', 'in', 'vector', 'space'], ['deep', 'learning', 'techniques', 'such', 'as', 'word', 'vec', 'and', 'glove', 'are', 'widely', 'used', 'in', 'nlp', 'applications'], ['this', 'is', 'sample', 'document', 'for', 'generating', 'word', 'embeddings'], ['another', 'example', 'document', 'is', 'provided', 'for', 'demonstration', 'purposes']]\n\nTraining Word2Vec model...\n\nWord2Vec: Similar words to 'document'\n[('semantic', 0.25275734066963196), ('example', 0.20082315802574158), ('vec', 0.19600167870521545), ('this', 0.1760866791009903), ('in', 0.17080651223659515)]\n\nLoading GloVe embeddings...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/4120915818.py:20: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n  glove2word2vec(glove_file, \"glove.6B.100d.word2vec.txt\")\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/4120915818.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nLoading GloVe embeddings...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mglove_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"glove.6B.100d.txt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mglove2word2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglove_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"glove.6B.100d.word2vec.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mglove_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"glove.6B.100d.word2vec.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nGloVe: Similar words to 'document'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/utils.py\u001b[0m in \u001b[0;36mnew_func1\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1519\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m                 )\n\u001b[0;32m-> 1521\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1523\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_func1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/scripts/glove2word2vec.py\u001b[0m in \u001b[0;36mglove2word2vec\u001b[0;34m(glove_input_file, word2vec_output_file)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \"\"\"\n\u001b[0;32m--> 109\u001b[0;31m     \u001b[0mglovekv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglove_input_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_header\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0mnum_lines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglovekv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglovekv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[1;32m   1717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1718\u001b[0m         \"\"\"\n\u001b[0;32m-> 1719\u001b[0;31m         return _load_word2vec_format(\n\u001b[0m\u001b[1;32m   1720\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1721\u001b[0m             \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_header\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mno_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[1;32m   2046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2047\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loading projection weights from %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2048\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2049\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mno_header\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2050\u001b[0m             \u001b[0;31m# deduce both vocab_size & vector_size from 1st pass over file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, compression, transport_params)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mtransport_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m     fobj = _shortcut_open(\n\u001b[0m\u001b[1;32m    178\u001b[0m         \u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36m_shortcut_open\u001b[0;34m(uri, mode, compression, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m    373\u001b[0m         \u001b[0mopen_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'errors'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'glove.6B.100d.txt'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'glove.6B.100d.txt'","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"!pip install -q gensim\n!wget -q http://nlp.stanford.edu/data/glove.6B.zip\n!unzip -q glove.6B.zip glove.6B.100d.txt\nimport gensim\nfrom gensim.utils import simple_preprocess\nfrom gensim.models import Word2Vec, KeyedVectors\nfrom gensim.scripts.glove2word2vec import glove2word2vec\ncorpus = [\n    \"Natural language processing enables computers to understand human language.\",\n    \"Word embeddings capture semantic relationships between words in a vector space.\",\n    \"Deep learning techniques such as Word2Vec and GloVe are widely used in NLP applications.\",\n    \"This is a sample document for generating word embeddings.\",\n    \"Another example document is provided for demonstration purposes.\"\n]\n\ntokenized_corpus = [simple_preprocess(sentence) for sentence in corpus]\nprint(\"Sample tokenized sentences:\\n\", tokenized_corpus)\n\nprint(\"\\nTraining Word2Vec model...\")\nw2v_model = Word2Vec(sentences=tokenized_corpus, vector_size=100, window=5, sg=1, min_count=1, workers=4)\nprint(\"\\nWord2Vec: Similar words to 'document'\")\nprint(w2v_model.wv.most_similar(\"document\", topn=5))\n\nglove2word2vec(\"glove.6B.100d.txt\", \"glove.6B.100d.word2vec.txt\")\nglove_model = KeyedVectors.load_word2vec_format(\"glove.6B.100d.word2vec.txt\", binary=False)\nprint(\"\\nGloVe: Similar words to 'document'\")\nprint(glove_model.most_similar(\"document\", topn=5))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# C. SENTENCE EMBEDDINGS WITH UNIVERSAL SENTENCE ENCODER(google collab)","metadata":{"editable":false}},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_hub as hub\nimport numpy as np\nsentences = [\n    \"This is a sentence.\",\n    \"Another example sentence.\",\n    \"Machine learning is fascinating.\",\n    \"I love natural language processing.\",\n    \"The sky is blue today.\"\n]\nprint(\"Loading Universal Sentence Encoder model...\")\nembed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\nprint(\"Model loaded!\")\nsentence_embeddings = embed(sentences)\nprint(f\"Sentence Embeddings Shape: {sentence_embeddings.shape}\\n\")\nfor i, sentence in enumerate(sentences):\n    print(f\"Sentence: {sentence}\")\n    print(f\"Embedding vector (first 5 values): {sentence_embeddings[i][:5].numpy()}\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T04:39:16.277247Z","iopub.execute_input":"2025-09-18T04:39:16.277600Z","iopub.status.idle":"2025-09-18T04:39:48.327657Z","shell.execute_reply.started":"2025-09-18T04:39:16.277570Z","shell.execute_reply":"2025-09-18T04:39:48.326289Z"},"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null}]}